학습이 된다면, 그 동안 이미지 튜닝하는 방법 혹은 csv 파일 수정 필요
bbox coder, 

train/d5203efda6708309bae1c60534f16192.png
val/d52590d27ebd08d17277ef794e31337f.png

1. 파일 복사해서 새로 만들기
2. htc_r50_fpn_1x_coco.py : train_pipeline, test_pipeline > RandomFlip // annotation
※ data_root = 'data/coco/'
   data > seg_prefix > data_root + 'stuffthingmaps/train2017/' // 의미...??? >> 주석처리함
3. htc_without_semantic_r50_fpn_1x_coco.py : _base_ > ../_base_/ >> /_base_/
   test_cfg > rcnn > iou_threshold : 0.5 > 0.4
4. coco_instance.py : dataset_type : CocoDataset > XrayDataset
   train_pipeline, test_pipeline : RandomFlip // annotation
   data : sample_per_gpu = 2 > 1, workers_per_gpu = 2 > 4
	train, val, test > ann_file = data_root + 'annotations/instances_train, val, test .json' 으로 변경
		         img_prefix = data_root + 'train, test, val /' 로 변경

/이미지 담기
- resize 해주긴 하니깐 본래 png 쓸 때는 그대로 넣고, chest 중앙만 추출할 경우 크기를 조금 줄이자.
/custom dataset 만들기
- with open ... 으로 모두 작성해야함.

>> Chapter 1. Config Name Style
filename 형태
{model}_[model setting]_{backbone}_{neck}_[norm setting]_[misc]_[gpu x batch_per_gpu]_{schedule}_{dataset}
설명
{model} detectors
[model setting] without_semantic
{backbone} x101
{neck} fpn
[norm_setting] bn
[misc] dconv, gcb, attention, albu, mstrain, ... 중 하나 (mstrain)
[gpu x batch_per_gpu] 1x2, 1x4, 1x8 중 하나 (높여봐야함)
{schedule} 1x, 2x, 20e, ... (12 epo, 24 epo, 계층모델 20 epo)
{dataset} coco, cityspaces, voc_0712, wider_face 같은 데이터셋
※ 이미지 최대 크기 : (1333, 800)

>> Chapter 2. Customize Datasets
설명 https://www.youtube.com/watch?v=h6s61a_pqfM&ab_channel=ImmersiveLimit
image : file_name, height, width, id
annotations : id - 고유번호, category_id - 클래스 번호, image_id - 이미지 번호
※ https://github.com/open-mmlab/mmdetection/blob/master/docs/tutorials/customize_dataset.md
categories : name and their id ( ex. {'id': 0, 'name': 'car'} )

데이터 전처리 후, coco format으로 customizing
1단계. customized dataset을 사용하기 위해 config 파일 수정 (
2단계. customized dataset 의 annotation 확인

#
mmdet/models/backbones/resnet.py > 131 > None 처리 for using DetectoRS R-CNN
